{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w = torch.tensor([2,-3.4])\n",
    "true_b = 4.2\n",
    "features,labels = d2l.synthetic_data(true_w,true_b,1000)   \n",
    "# 之前的生成数据的方式一个封装，相当于围绕真实的w，b生成一组扰动的数据(一千个)，平时也可以用到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成线性回归的扰动数据的情况\n",
    "# fea = features.numpy()\n",
    "# Data = pd.DataFrame(fea.tolist())\n",
    "# Data.to_excel(\"从tensor转化为excel.xlsx\",index=False)\n",
    "# 思考一下怎么去生成更多的变量的扰动数据"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在深度学习中怎么去读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_array(data_arrays,batch_size,is_train = True):\n",
    "    \"\"\"Pytorch中数据迭代器的构造\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset,batch_size,shuffle=is_train)   # 每个epoch要不要重新洗牌\n",
    "# 这里is_train的意义是什么？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10      # 每次操作中一次的数量\n",
    "data_iter = load_array((features,labels),batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.9673, -1.8430],\n",
       "         [-0.2685,  1.0562],\n",
       "         [ 1.4011, -0.0946],\n",
       "         [ 1.2214, -1.9569],\n",
       "         [-0.4392, -0.1399],\n",
       "         [-0.4990,  0.9791],\n",
       "         [-1.0206,  1.0568],\n",
       "         [ 0.3724,  0.6013],\n",
       "         [-0.1423,  0.4120],\n",
       "         [-0.0350,  0.6601]]),\n",
       " tensor([[12.3855],\n",
       "         [ 0.0814],\n",
       "         [ 7.3046],\n",
       "         [13.3014],\n",
       "         [ 3.7905],\n",
       "         [-0.1293],\n",
       "         [-1.4434],\n",
       "         [ 2.9059],\n",
       "         [ 2.4975],\n",
       "         [ 1.8929]])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用iter构造python迭代器，并且使用next从迭代器中获取第一项\n",
    "next(iter(data_iter))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在$Pytorch$中，全连接层在Linear类中定义\n",
    "\n",
    "要向`nn.Linear`中传递两个参数\n",
    "一个是$输入特征的形状$\n",
    "另一个是$输出特征的形状$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "net = nn.Sequential(nn.Linear(2,1))   # 这里的(2,1)即是输入特征的形状和输出特征的形状"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在使用net之前，我们需要对模型的参数进行一个初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过net[0]选择网络中的第一个图层\n",
    "net[0].weight.data.normal_(0,0.01)\n",
    "net[0].bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数，平方2阶范数，默认情况下将返回所有样本损失的平均值\n",
    "loss = nn.MSELoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小批量随机梯度下降**MBGD**算法是一种优化神经网络的工具\n",
    "\n",
    "$PyTorch$在`optim`模块中实现了该算法的很多变种\n",
    "\n",
    "对于MBGD算法只要指定学习率lr值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = torch.optim.SGD(net.parameters(),lr=0.03)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在每个迭代周期里，我们将完整遍历一次数据集(train_data)，不停地从中获取一个小批量的输入和响应的标签\n",
    "\n",
    "对于每一个**小批量**\n",
    "\n",
    "- 通过调用`net(X)`生成预测并计算损失l(前向传播$forward$)\n",
    "- 通过进行反向传播($backward$)来计算**梯度**\n",
    "- 通过调用优化器来更新模型的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1,loss0.000213\n",
      "epoch2,loss0.000104\n",
      "epoch3,loss0.000104\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for X,y in data_iter:\n",
    "        l = loss(net(X),y)\n",
    "        trainer.zero_grad()   # 梯度清零\n",
    "        l.backward()          # 反向传播\n",
    "        trainer.step()        # 优化器对x(X)的值进行更新，x=x-lr*x.grad 沿着梯度的反方向调整变量值\n",
    "    l = loss(net(features),labels)\n",
    "    print(f'epoch{epoch+1},loss{l:f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w的估计误差: tensor([-1.6689e-05,  7.0095e-04])\n",
      "b的估计误差: tensor([0.0003])\n"
     ]
    }
   ],
   "source": [
    "w = net[0].weight.data\n",
    "print('w的估计误差:',true_w - w.reshape(true_w.shape))\n",
    "b = net[0].bias.data\n",
    "print('b的估计误差:',true_b - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e9712843e66ca380271771672d27c21094ede3ab9c2e2a64b49787fb387d26ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
